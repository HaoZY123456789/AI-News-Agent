#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æµ‹è¯•å…³é”®è¯å’ŒAIæ€»ç»“åŠŸèƒ½
"""

import sys
sys.path.append('.')

from news_scraper import AINewsScraper
from email_sender import EmailSender
from database import NewsDatabase
from datetime import datetime

def test_keywords_and_summary():
    """æµ‹è¯•å…³é”®è¯æ£€æµ‹å’ŒAIæ€»ç»“åŠŸèƒ½"""
    
    # åˆ›å»ºæµ‹è¯•æ–‡ç« æ•°æ®
    test_articles = [
        {
            'title': 'DeepSeekæ¨å‡ºæ–°ä¸€ä»£å¤§è¯­è¨€æ¨¡å‹ï¼Œæ€§èƒ½è¶…è¶ŠGPT-4',
            'link': 'https://example.com/article1',
            'summary': 'DeepSeekå…¬å¸ä»Šæ—¥å‘å¸ƒäº†æœ€æ–°çš„å¤§è¯­è¨€æ¨¡å‹DeepSeek-V3ï¼Œè¯¥æ¨¡å‹åœ¨å¤šé¡¹è¯„æµ‹ä¸­è¡¨ç°å‡ºè‰²ï¼Œåœ¨ä»£ç ç”Ÿæˆã€æ¨ç†èƒ½åŠ›ç­‰æ–¹é¢è¶…è¶Šäº†OpenAIçš„GPT-4æ¨¡å‹ã€‚æ–°æ¨¡å‹é‡‡ç”¨äº†å…ˆè¿›çš„Transformeræ¶æ„å’Œåˆ›æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œå‚æ•°è§„æ¨¡è¾¾åˆ°1750äº¿ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥è¾“å‡ºã€‚',
            'published': datetime.now(),
            'source': 'TechCrunch',
            'content_hash': hash('test1')
        },
        {
            'title': 'è…¾è®¯AIå®éªŒå®¤å‘å¸ƒæ™ºèƒ½ä½“æ¡†æ¶Qoderï¼ŒåŠ©åŠ›ç¼–ç¨‹æ•ˆç‡æå‡',
            'link': 'https://example.com/article2',
            'summary': 'è…¾è®¯AIå®éªŒå®¤å®£å¸ƒå¼€æºæ™ºèƒ½ä½“å¼€å‘æ¡†æ¶Qoderï¼Œè¯¥æ¡†æ¶ä¸“ä¸ºè½¯ä»¶å¼€å‘åœºæ™¯è®¾è®¡ï¼Œé›†æˆäº†ä»£ç ç”Ÿæˆã€è°ƒè¯•ã€æµ‹è¯•ç­‰å¤šé¡¹åŠŸèƒ½ã€‚QoderåŸºäºå¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼Œèƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€éœ€æ±‚å¹¶è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ä»£ç ï¼Œæ˜¾è‘—æå‡å¼€å‘æ•ˆç‡ã€‚',
            'published': datetime.now(),
            'source': '36æ°ª',
            'content_hash': hash('test2')
        },
        {
            'title': 'Trae AIå¹³å°è·å¾—åƒä¸‡ç¾å…ƒèèµ„ï¼Œä¸“æ³¨ä¼ä¸šçº§AIè§£å†³æ–¹æ¡ˆ',
            'link': 'https://example.com/article3',
            'summary': 'AIåˆåˆ›å…¬å¸Traeä»Šæ—¥å®£å¸ƒå®Œæˆåƒä¸‡ç¾å…ƒAè½®èèµ„ï¼Œæœ¬è½®èèµ„ç”±çŸ¥åé£æŠ•æœºæ„é¢†æŠ•ã€‚Traeä¸“æ³¨äºä¸ºä¼ä¸šæä¾›å®šåˆ¶åŒ–çš„äººå·¥æ™ºèƒ½è§£å†³æ–¹æ¡ˆï¼Œå…¶å¹³å°é›†æˆäº†æœºå™¨å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰å¤šé¡¹æŠ€æœ¯ï¼Œå·²æœåŠ¡è¶…è¿‡100å®¶ä¼ä¸šå®¢æˆ·ã€‚',
            'published': datetime.now(),
            'source': 'VentureBeat',
            'content_hash': hash('test3')
        }
    ]
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å…³é”®è¯å’ŒAIæ€»ç»“åŠŸèƒ½...")
    print("=" * 50)
    
    # åˆå§‹åŒ–æŠ“å–å™¨
    scraper = AINewsScraper()
    
    # 1. æµ‹è¯•å…³é”®è¯è¿‡æ»¤
    print("\nğŸ“‹ æ­¥éª¤1ï¼šæµ‹è¯•å…³é”®è¯è¿‡æ»¤...")
    filtered_articles = scraper.filter_ai_keywords(test_articles)
    print(f"åŸå§‹æ–‡ç« æ•°é‡ï¼š{len(test_articles)}")
    print(f"è¿‡æ»¤åæ–‡ç« æ•°é‡ï¼š{len(filtered_articles)}")
    
    for i, article in enumerate(filtered_articles, 1):
        print(f"\næ–‡ç«  {i}ï¼š{article['title']}")
        print(f"æ£€æµ‹åˆ°çš„å…³é”®è¯ï¼š{article.get('detected_keywords', [])}")
    
    # 2. æµ‹è¯•AIæ€»ç»“
    print("\nğŸ“ æ­¥éª¤2ï¼šæµ‹è¯•AIæ€»ç»“ç”Ÿæˆ...")
    summarized_articles = scraper.summarize_articles_batch(filtered_articles)
    
    for i, article in enumerate(summarized_articles, 1):
        print(f"\næ–‡ç«  {i} AIæ€»ç»“ï¼š")
        print("-" * 30)
        print(article.get('ai_summary', 'æ— æ€»ç»“'))
    
    # 3. æµ‹è¯•é‚®ä»¶æ¨¡æ¿
    print("\nğŸ“§ æ­¥éª¤3ï¼šæµ‹è¯•é‚®ä»¶æ¨¡æ¿...")
    email_sender = EmailSender()
    html_content = email_sender.create_email_content(summarized_articles)
    
    # å°†HTMLå†…å®¹ä¿å­˜åˆ°æ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥çœ‹
    with open('test_email_preview.html', 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print("é‚®ä»¶HTMLé¢„è§ˆå·²ä¿å­˜åˆ°ï¼štest_email_preview.html")
    
    # 4. æµ‹è¯•æ•°æ®åº“å­˜å‚¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    print("\nğŸ’¾ æ­¥éª¤4ï¼šæµ‹è¯•æ•°æ®åº“å­˜å‚¨...")
    db = NewsDatabase()
    
    # ç›´æ¥ä½¿ç”¨æ‰¹é‡æ·»åŠ æ–¹æ³•
    added_count = db.add_articles(summarized_articles)
    print(f"æˆåŠŸæ·»åŠ  {added_count} ç¯‡æ–‡ç« åˆ°æ•°æ®åº“")
    
    # éªŒè¯æ•°æ®åº“ä¸­çš„æ•°æ®
    unsent_articles = db.get_unsent_articles()
    print(f"æ•°æ®åº“ä¸­å¾…å‘é€æ–‡ç« æ•°é‡ï¼š{len(unsent_articles)}")
    
    if unsent_articles:
        print("\næ•°æ®åº“ä¸­ç¬¬ä¸€ç¯‡æ–‡ç« çš„å…³é”®è¯å’Œæ€»ç»“ï¼š")
        first_article = unsent_articles[0]
        print(f"å…³é”®è¯ï¼š{first_article.get('detected_keywords', '')}")
        print(f"AIæ€»ç»“é•¿åº¦ï¼š{len(first_article.get('ai_summary', ''))}")
        print(f"AIæ€»ç»“å‰100å­—ç¬¦ï¼š{first_article.get('ai_summary', '')[:100]}...")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“ï¼š")
    print(f"- å…³é”®è¯è¿‡æ»¤ï¼š{'âœ… æ­£å¸¸' if len(filtered_articles) > 0 else 'âŒ å¼‚å¸¸'}")
    print(f"- AIæ€»ç»“ç”Ÿæˆï¼š{'âœ… æ­£å¸¸' if all('ai_summary' in article for article in summarized_articles) else 'âŒ å¼‚å¸¸'}")
    print(f"- æ•°æ®åº“å­˜å‚¨ï¼š{'âœ… æ­£å¸¸' if added_count > 0 else 'âŒ å¼‚å¸¸'}")
    print(f"- é‚®ä»¶æ¨¡æ¿ï¼š{'âœ… æ­£å¸¸' if len(html_content) > 1000 else 'âŒ å¼‚å¸¸'}")

if __name__ == "__main__":
    test_keywords_and_summary()