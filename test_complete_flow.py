#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´æµç¨‹æµ‹è¯•è„šæœ¬ - æµ‹è¯•æ–‡ç« æ€»ç»“åŠŸèƒ½
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from news_scraper import AINewsScraper
from database import NewsDatabase
from email_sender import EmailSender
import sqlite3

def test_complete_flow():
    """æµ‹è¯•å®Œæ•´çš„æ–‡ç« å¤„ç†æµç¨‹"""
    print("ğŸ§ª æµ‹è¯•AIèµ„è®¯æ™ºèƒ½ä½“å®Œæ•´æµç¨‹")
    print("=" * 50)
    
    # åˆå§‹åŒ–ç»„ä»¶
    scraper = AINewsScraper()
    db = NewsDatabase()
    email_sender = EmailSender()
    
    print("æ­¥éª¤1: æ¸…ç†ç°æœ‰æœªå‘é€æ–‡ç« ä»¥ä¾¿é‡æ–°æµ‹è¯•...")
    try:
        with sqlite3.connect(db.db_path) as conn:
            cursor = conn.cursor()
            # åˆ é™¤æ‰€æœ‰æœªå‘é€çš„æ–‡ç« 
            cursor.execute('DELETE FROM articles WHERE is_sent = 0')
            deleted_count = cursor.rowcount
            conn.commit()
            print(f"   æ¸…ç†äº† {deleted_count} ç¯‡æœªå‘é€æ–‡ç« ")
    except Exception as e:
        print(f"   æ¸…ç†å¤±è´¥: {e}")
    
    print("\næ­¥éª¤2: æŠ“å–AIèµ„è®¯...")
    all_articles = scraper.get_ai_news()
    print(f"   åŸå§‹æ–‡ç« æ•°é‡: {len(all_articles)}")
    
    print("\næ­¥éª¤3: è¿‡æ»¤AIç›¸å…³æ–‡ç« ...")
    filtered_articles = scraper.filter_ai_keywords(all_articles)
    print(f"   è¿‡æ»¤åæ–‡ç« æ•°é‡: {len(filtered_articles)}")
    
    print("\næ­¥éª¤4: å¯¹æ–‡ç« è¿›è¡Œæ™ºèƒ½æ€»ç»“...")
    summarized_articles = scraper.summarize_articles_batch(filtered_articles)
    print(f"   å®Œæˆæ€»ç»“çš„æ–‡ç« æ•°é‡: {len(summarized_articles)}")
    
    # æ˜¾ç¤ºå‰å‡ ç¯‡æ–‡ç« çš„æ€»ç»“ç¤ºä¾‹
    if summarized_articles:
        print("\nğŸ“„ æ€»ç»“ç¤ºä¾‹:")
        for i, article in enumerate(summarized_articles[:2], 1):
            print(f"\næ–‡ç«  {i}:")
            print(f"   æ ‡é¢˜: {article['title'][:50]}...")
            print(f"   å…³é”®è¯: {', '.join(article.get('detected_keywords', [])[:3])}")
            if article.get('ai_summary'):
                summary_lines = article['ai_summary'].split('\n')
                print(f"   æ€»ç»“: {summary_lines[0] if summary_lines else 'æ— '}...")
    
    print("\næ­¥éª¤5: ä¿å­˜åˆ°æ•°æ®åº“...")
    unique_articles = db.get_duplicate_check_results(summarized_articles)
    new_count = db.add_articles(unique_articles)
    print(f"   æ–°å¢æ–‡ç« æ•°é‡: {new_count}")
    
    print("\næ­¥éª¤6: æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ–‡ç« ...")
    unsent_articles = db.get_unsent_articles(limit=3)
    print(f"   æœªå‘é€æ–‡ç« æ•°é‡: {len(unsent_articles)}")
    
    if unsent_articles:
        print("\nğŸ“° æ•°æ®åº“ä¸­çš„æ–‡ç« ä¿¡æ¯:")
        for i, article in enumerate(unsent_articles, 1):
            print(f"\næ–‡ç«  {i}:")
            print(f"   æ ‡é¢˜: {article['title']}")
            print(f"   å…³é”®è¯: {', '.join(article.get('detected_keywords', []))}")
            print(f"   æ€»ç»“çŠ¶æ€: {'å·²ç”Ÿæˆ' if article.get('ai_summary') else 'æœªç”Ÿæˆ'}")
            
            if article.get('ai_summary'):
                print(f"   æ€»ç»“é¢„è§ˆ: {article['ai_summary'][:100]}...")
    
    print("\næ­¥éª¤7: æµ‹è¯•é‚®ä»¶å‘é€...")
    if unsent_articles:
        try:
            success = email_sender.send_news_email(unsent_articles[:2])  # åªå‘é€å‰2ç¯‡æµ‹è¯•
            if success:
                print("   âœ… é‚®ä»¶å‘é€æˆåŠŸï¼")
                print("   è¯·æ£€æŸ¥æ‚¨çš„é‚®ç®±ï¼Œç¡®è®¤æ–‡ç« æ€»ç»“æ˜¯å¦æ­£ç¡®æ˜¾ç¤º")
            else:
                print("   âŒ é‚®ä»¶å‘é€å¤±è´¥")
        except Exception as e:
            print(f"   âŒ é‚®ä»¶å‘é€å‡ºé”™: {e}")
    else:
        print("   âš ï¸  æ²¡æœ‰æ–‡ç« å¯å‘é€")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ å®Œæ•´æµç¨‹æµ‹è¯•å®Œæˆï¼")
    print("è¯·æ£€æŸ¥æ‚¨çš„é‚®ç®±ï¼Œç¡®è®¤æ”¶åˆ°çš„é‚®ä»¶ä¸­æ˜¯å¦åŒ…å«:")
    print("â€¢ ğŸ·ï¸ æ£€æµ‹åˆ°çš„AIå…³é”®è¯")
    print("â€¢ ğŸ¤– AIæ™ºèƒ½æ€»ç»“å†…å®¹")

if __name__ == "__main__":
    test_complete_flow()