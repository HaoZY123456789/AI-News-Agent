import requests
import feedparser
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import time
import configparser

class AINewsScraper:
    """AIèµ„è®¯æŠ“å–å™¨"""
    
    def __init__(self, config_file: str = 'config.ini'):
        self.config = configparser.ConfigParser()
        self.config.read(config_file, encoding='utf-8')
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('scraper.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # RSSæºé…ç½®
        self.rss_sources = dict(self.config.items('sources'))
        
    def fetch_rss_feed(self, url: str, source_name: str) -> List[Dict]:
        """æŠ“å–RSSæºæ•°æ®"""
        articles = []
        try:
            self.logger.info(f"æ­£åœ¨æŠ“å– {source_name}: {url}")
            
            # è®¾ç½®è¶…æ—¶å’Œé‡è¯•
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # è§£æRSS
            feed = feedparser.parse(response.content)
            
            for entry in feed.entries:
                # è§£æå‘å¸ƒæ—¶é—´
                published_time = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    published_time = datetime(*entry.published_parsed[:6])
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    published_time = datetime(*entry.updated_parsed[:6])
                else:
                    published_time = datetime.now()
                
                # åªè·å–æœ€è¿‘10å°æ—¶çš„æ–‡ç« 
                if published_time and (datetime.now() - published_time).total_seconds() > 10 * 3600:
                    continue
                
                # æå–æ‘˜è¦ï¼Œå¤„ç†HTMLæ ‡ç­¾
                summary = ""
                if hasattr(entry, 'summary'):
                    soup = BeautifulSoup(entry.summary, 'html.parser')
                    summary = soup.get_text().strip()[:300] + "..."
                
                article = {
                    'title': entry.title,
                    'link': entry.link,
                    'summary': summary,
                    'published': published_time,
                    'source': source_name,
                    'content_hash': hash(entry.title + entry.link)  # ç”¨äºå»é‡
                }
                
                articles.append(article)
                
            self.logger.info(f"ä» {source_name} è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            
        except Exception as e:
            self.logger.error(f"æŠ“å– {source_name} æ—¶å‡ºé”™: {str(e)}")
            
        return articles
    
    def scrape_web_article(self, url: str) -> Optional[str]:
        """æŠ“å–ç½‘é¡µæ–‡ç« å†…å®¹ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        try:
            response = requests.get(url, headers=self.headers, timeout=20)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # å°è¯•æ‰¾åˆ°æ–‡ç« å†…å®¹
            content_selectors = [
                'article', '.article-content', '.post-content', 
                '.entry-content', '.content', 'main'
            ]
            
            content = ""
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    content = element.get_text().strip()
                    break
            
            # æ¸…ç†å†…å®¹ï¼Œé™åˆ¶é•¿åº¦
            if content:
                content = ' '.join(content.split())[:500] + "..."
                
            return content
            
        except Exception as e:
            self.logger.error(f"æŠ“å–ç½‘é¡µå†…å®¹å¤±è´¥ {url}: {str(e)}")
            return None
    
    def get_ai_news(self) -> List[Dict]:
        """è·å–æ‰€æœ‰AIèµ„è®¯"""
        all_articles = []
        
        for source_name, url in self.rss_sources.items():
            articles = self.fetch_rss_feed(url, source_name)
            all_articles.extend(articles)
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        all_articles.sort(key=lambda x: x['published'], reverse=True)
        
        self.logger.info(f"æ€»å…±è·å–åˆ° {len(all_articles)} ç¯‡AIèµ„è®¯")
        return all_articles
    
    def filter_ai_keywords(self, articles: List[Dict]) -> List[Dict]:
        """æ ¹æ®AIå…³é”®è¯è¿‡æ»¤æ–‡ç« ï¼ˆå¤šå±‚ç­›é€‰ï¼Œæ›´ç²¾å‡†ï¼‰"""
        
        # ç¬¬ä¸€å±‚ï¼šé«˜ä¼˜å…ˆçº§å…³é”®è¯ï¼ˆæ–°å·¥å…·ã€æ–°æ¨¡å‹å‘å¸ƒç›¸å…³ï¼‰
        high_priority_keywords = [
            # æ–°AIå·¥å…·/å¹³å°å‘å¸ƒ
            'released', 'launched', 'introduced', 'unveiled', 'announced', 'debuts',
            'å‘å¸ƒ', 'æ¨å‡º', 'ä¸Šçº¿', 'å‘å¸ƒä¼š', 'æ­£å¼å‘å¸ƒ', 'å®£å¸ƒ', 'é¦–å‘', 'å¼€æº',
            
            # æ–°å¤§æ¨¡å‹ç›¸å…³
            'new model', 'latest model', 'breakthrough model', 'next-generation',
            'æ–°æ¨¡å‹', 'æœ€æ–°æ¨¡å‹', 'æ–°ä¸€ä»£', 'çªç ´æ€§æ¨¡å‹', 'å…¨æ–°æ¨¡å‹',
            
            # ç‰ˆæœ¬æ›´æ–°
            'version', 'v2', 'v3', 'v4', '2.0', '3.0', '4.0', 'update', 'upgrade',
            'ç‰ˆæœ¬', 'å‡çº§', 'æ›´æ–°', 'beta', 'alpha',
            
            # æŠ€æœ¯çªç ´
            'breakthrough', 'milestone', 'achievement', 'innovation', 'revolutionary',
            'çªç ´', 'é‡Œç¨‹ç¢‘', 'åˆ›æ–°', 'é©å‘½æ€§', 'é¢ è¦†æ€§',
        ]
        
        # ç¬¬äºŒå±‚ï¼šæ ¸å¿ƒAIæŠ€æœ¯å…³é”®è¯
        core_ai_keywords = [
            # å¤§æ¨¡å‹ç›¸å…³
            'gpt', 'llm', 'large language model', 'transformer', 'chatgpt', 'claude', 
            'gemini', 'llama', 'bert', 'palm', 'deepseek', 'qwen', 'baichuan',
            'å¤§è¯­è¨€æ¨¡å‹', 'å¤§æ¨¡å‹', 'ç”Ÿæˆå¼AI', 'å¯¹è¯æ¨¡å‹',
            
            # AIå·¥å…·å¹³å°
            'copilot', 'cursor', 'qoder', 'trae', 'midjourney', 'dalle', 'stable diffusion',
            'sora', 'runway', 'luma', 'pika', 'kling', 'æ–‡å¿ƒä¸€è¨€', 'é€šä¹‰åƒé—®', 'kimi',
            
            # æŠ€æœ¯å…¬å¸/ç»„ç»‡
            'openai', 'anthropic', 'google ai', 'deepmind', 'microsoft ai', 'meta ai',
            'nvidia', 'hugging face', 'ç™¾åº¦', 'é˜¿é‡Œå·´å·´', 'è…¾è®¯', 'å­—èŠ‚è·³åŠ¨', 'åä¸º',
            'ç§‘å¤§è®¯é£', 'å•†æ±¤', 'æ—·è§†', 'æ™ºè°±AI', 'æœˆä¹‹æš—é¢', 'é¢å£æ™ºèƒ½',
            
            # AIåº”ç”¨é¢†åŸŸ
            'artificial intelligence', 'machine learning', 'deep learning', 'neural network',
            'computer vision', 'natural language processing', 'generative ai', 'multimodal',
            'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'è®¡ç®—æœºè§†è§‰', 
            'è‡ªç„¶è¯­è¨€å¤„ç†', 'å¤šæ¨¡æ€', 'æ™ºèƒ½ä½“', 'agent',
        ]
        
        # ç¬¬ä¸‰å±‚ï¼šæ’é™¤å…³é”®è¯ï¼ˆé™ä½å™ªéŸ³ï¼‰
        exclude_keywords = [
            'advertisement', 'promotion', 'marketing', 'sponsored', 'affiliate',
            'å¹¿å‘Š', 'æ¨å¹¿', 'è¥é”€', 'èµåŠ©', 'è”ç›Ÿ',
            'tutorial', 'how to', 'guide', 'tips', 'tricks',
            'æ•™ç¨‹', 'å¦‚ä½•', 'æŒ‡å—', 'æŠ€å·§', 'æ”»ç•¥',
        ]
        
        filtered_articles = []
        for article in articles:
            title = article['title']
            summary = article['summary']
            content = f"{title} {summary}"
            content_lower = content.lower()
            
            # è®¡ç®—æ–‡ç« ç›¸å…³æ€§åˆ†æ•°
            relevance_score = 0
            detected_keywords = []
            
            # æ£€æŸ¥é«˜ä¼˜å…ˆçº§å…³é”®è¯ï¼ˆæƒé‡3ï¼‰
            for keyword in high_priority_keywords:
                if keyword.lower() in content_lower:
                    relevance_score += 3
                    detected_keywords.append(keyword)
            
            # æ£€æŸ¥æ ¸å¿ƒAIå…³é”®è¯ï¼ˆæƒé‡1ï¼‰
            for keyword in core_ai_keywords:
                if keyword.lower() in content_lower:
                    relevance_score += 1
                    detected_keywords.append(keyword)
            
            # æ£€æŸ¥æ’é™¤å…³é”®è¯ï¼ˆè´Ÿæƒé‡ï¼‰
            for keyword in exclude_keywords:
                if keyword.lower() in content_lower:
                    relevance_score -= 2
            
            # é¢å¤–åŠ åˆ†é¡¹ï¼šæ ‡é¢˜ä¸­åŒ…å«å…³é”®è¯
            title_lower = title.lower()
            if any(keyword.lower() in title_lower for keyword in high_priority_keywords):
                relevance_score += 2
            
            # ç­›é€‰æ¡ä»¶ï¼šç›¸å…³æ€§åˆ†æ•° >= 4 ä¸”åŒ…å«è‡³å°‘ä¸€ä¸ªæ ¸å¿ƒå…³é”®è¯
            has_core_keyword = any(keyword.lower() in content_lower for keyword in core_ai_keywords)
            
            if relevance_score >= 4 and has_core_keyword:
                # å»é‡å¹¶é™åˆ¶å…³é”®è¯æ•°é‡
                unique_keywords = list(dict.fromkeys(detected_keywords))  # ä¿æŒé¡ºåºå»é‡
                article['detected_keywords'] = unique_keywords[:5]
                article['relevance_score'] = relevance_score
                filtered_articles.append(article)
        
        # æŒ‰ç›¸å…³æ€§åˆ†æ•°é™åºæ’åº
        filtered_articles.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        self.logger.info(f"ä½¿ç”¨ç²¾å‡†ç­›é€‰ç­–ç•¥ï¼Œè¿‡æ»¤åå‰©ä½™ {len(filtered_articles)} ç¯‡é«˜è´¨é‡AIæ–‡ç« ")
        
        # è¾“å‡ºå‰å‡ ç¯‡æ–‡ç« çš„å¾—åˆ†ç”¨äºè°ƒè¯•
        for i, article in enumerate(filtered_articles[:3]):
            self.logger.info(f"æ–‡ç« {i+1}: å¾—åˆ†{article.get('relevance_score', 0)} - {article['title'][:50]}...")
            
        return filtered_articles
    
    def summarize_article(self, article: Dict) -> str:
        """å¯¹å•ç¯‡æ–‡ç« è¿›è¡Œç²¾ç®€æ™ºèƒ½æ€»ç»“"""
        try:
            title = article.get('title', '')
            summary = article.get('summary', '')
            source = article.get('source', '')
            keywords = article.get('detected_keywords', [])
            
            # åˆå¹¶æ ‡é¢˜å’Œæ‘˜è¦ç”¨äºåˆ†æ
            content = f"{title} {summary}"
            
            # æå–å…³é”®ä¿¡æ¯
            analysis_result = self._analyze_content_concise(content, keywords)
            
            # ç”Ÿæˆç²¾ç®€çš„ç»“æ„åŒ–æ€»ç»“
            summary_text = f"""
ğŸ“° {title}

ğŸ¯ æ ¸å¿ƒäº®ç‚¹ï¼š{analysis_result['key_insight']}

ğŸ“Š ä»·å€¼åˆ¤æ–­ï¼š{analysis_result['value_assessment']}

ğŸ“± æ¥æºï¼š{source}
"""
            
            return summary_text
            
        except Exception as e:
            self.logger.error(f"æ–‡ç« æ€»ç»“å¤±è´¥: {str(e)}")
            return f"æ–‡ç« æ ‡é¢˜: {article.get('title', '')}ã€‚æ€»ç»“ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"
    
    def _analyze_content_concise(self, content: str, keywords: List[str]) -> Dict[str, str]:
        """ç²¾ç®€åˆ†ææ–‡ç« å†…å®¹å¹¶æå–å…³é”®ä¿¡æ¯"""
        content_lower = content.lower()
        
        # æ ¸å¿ƒäº®ç‚¹æå–ï¼ˆä¸€å¥è¯æ¦‚æ‹¬ï¼‰
        key_insight = self._extract_key_insight(content, keywords)
        
        # ä»·å€¼åˆ¤æ–­ï¼ˆç®€æ´è¯„ä¼°ï¼‰
        value_assessment = self._assess_value(content, keywords)
        
        return {
            'key_insight': key_insight,
            'value_assessment': value_assessment
        }
    
    def _extract_key_insight(self, content: str, keywords: List[str]) -> str:
        """æå–æ ¸å¿ƒäº®ç‚¹ï¼ˆä¸€å¥è¯æ€»ç»“ï¼‰"""
        content_lower = content.lower()
        
        # æ ¹æ®å…³é”®è¯ç±»å‹ç”Ÿæˆç²¾ç®€äº®ç‚¹
        
        # æ–°æ¨¡å‹/å·¥å…·å‘å¸ƒ
        if any(kw in ['released', 'launched', 'unveiled', 'å‘å¸ƒ', 'æ¨å‡º', 'ä¸Šçº¿'] for kw in keywords):
            if any(kw in ['gpt', 'llm', 'å¤§è¯­è¨€æ¨¡å‹', 'å¤§æ¨¡å‹'] for kw in keywords):
                return "æ–°å¤§è¯­è¨€æ¨¡å‹æ­£å¼å‘å¸ƒï¼Œæ€§èƒ½å’Œèƒ½åŠ›æœ‰æ˜¾è‘—æå‡"
            elif any(kw in ['ai tool', 'platform', 'å¹³å°', 'å·¥å…·'] for kw in keywords):
                return "æ–°AIå·¥å…·/å¹³å°å‘å¸ƒï¼Œä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„AIä½“éªŒ"
            else:
                return "é‡è¦AIäº§å“æˆ–æŠ€æœ¯æ­£å¼å‘å¸ƒ"
        
        # æŠ€æœ¯çªç ´
        elif any(kw in ['breakthrough', 'milestone', 'çªç ´', 'é‡Œç¨‹ç¢‘', 'åˆ›æ–°'] for kw in keywords):
            if any(kw in ['performance', 'æ€§èƒ½', 'capability', 'èƒ½åŠ›'] for kw in keywords):
                return "æŠ€æœ¯æ€§èƒ½å®ç°é‡å¤§çªç ´ï¼Œè¶…è¶Šç°æœ‰æ°´å¹³"
            else:
                return "AIæŠ€æœ¯å–å¾—é‡è¦çªç ´æ€§è¿›å±•"
        
        # æŠ•èµ„èèµ„
        elif any(kw in ['investment', 'funding', 'èèµ„', 'æŠ•èµ„'] for kw in keywords):
            return "AIå…¬å¸è·å¾—é‡è¦èµ„æœ¬æ³¨å…¥ï¼Œä¿ƒè¿›æŠ€æœ¯å‘å±•"
        
        # ç‰ˆæœ¬æ›´æ–°
        elif any(kw in ['version', 'update', 'upgrade', 'ç‰ˆæœ¬', 'å‡çº§', 'æ›´æ–°'] for kw in keywords):
            return "äº§å“ç‰ˆæœ¬é‡å¤§æ›´æ–°ï¼ŒåŠŸèƒ½å’Œæ€§èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–"
        
        # åˆä½œä¼™ä¼´
        elif any(kw in ['partnership', 'collaboration', 'åˆä½œ', 'æˆ˜ç•¥è”ç›Ÿ'] for kw in keywords):
            return "AIé¢†åŸŸé‡è¦åˆä½œè¾¾æˆï¼Œæ¨åŠ¨è¡Œä¸šå‘å±•"
        
        # å¸‚åœºç«äº‰
        elif any(kw in ['competition', 'market', 'ç«äº‰', 'å¸‚åœº'] for kw in keywords):
            return "AIå¸‚åœºç«äº‰æ ¼å±€å‘ç”Ÿå˜åŒ–ï¼Œå½±å“è¡Œä¸šèµ°å‘"
        
        # æ”¿ç­–ç›‘ç®¡
        elif any(kw in ['policy', 'regulation', 'æ”¿ç­–', 'ç›‘ç®¡', 'æ³•è§„'] for kw in keywords):
            return "AIç›¸å…³æ”¿ç­–æˆ–ç›‘ç®¡å‡ºå°ï¼Œå½±å“è¡Œä¸šå‘å±•æ–¹å‘"
        
        # é»˜è®¤æƒ…å†µ
        else:
            return "AIé¢†åŸŸå€¼å¾—å…³æ³¨çš„é‡è¦åŠ¨æ€"
    
    def _assess_value(self, content: str, keywords: List[str]) -> str:
        """è¯„ä¼°æ–‡ç« ä»·å€¼ï¼ˆç®€æ´åˆ¤æ–­ï¼‰"""
        content_lower = content.lower()
        
        # é«˜ä»·å€¼å…³é”®è¯
        high_value_indicators = [
            'breakthrough', 'revolutionary', 'milestone', 'first-ever', 'unprecedented',
            'çªç ´', 'é©å‘½æ€§', 'é‡Œç¨‹ç¢‘', 'é¦–æ¬¡', 'å²ä¸Šé¦–æ¬¡', 'å‰æ‰€æœªæœ‰',
            'launched', 'released', 'unveiled', 'announced',
            'å‘å¸ƒ', 'æ¨å‡º', 'ä¸Šçº¿', 'æ­£å¼å‘å¸ƒ'
        ]
        
        # ä¸­ç­‰ä»·å€¼å…³é”®è¯
        medium_value_indicators = [
            'funding', 'investment', 'partnership', 'collaboration',
            'èèµ„', 'æŠ•èµ„', 'åˆä½œ', 'æˆ˜ç•¥è”ç›Ÿ',
            'update', 'upgrade', 'improvement',
            'æ›´æ–°', 'å‡çº§', 'æ”¹è¿›'
        ]
        
        # ä½ä»·å€¼å…³é”®è¯
        low_value_indicators = [
            'analysis', 'report', 'study', 'survey', 'tutorial',
            'åˆ†æ', 'æŠ¥å‘Š', 'ç ”ç©¶', 'è°ƒæŸ¥', 'æ•™ç¨‹'
        ]
        
        # è®¡ç®—ä»·å€¼åˆ†æ•°
        value_score = 0
        
        for indicator in high_value_indicators:
            if indicator.lower() in content_lower:
                value_score += 3
                
        for indicator in medium_value_indicators:
            if indicator.lower() in content_lower:
                value_score += 2
                
        for indicator in low_value_indicators:
            if indicator.lower() in content_lower:
                value_score -= 1
        
        # æ ¹æ®åˆ†æ•°è¿”å›ä»·å€¼åˆ¤æ–­
        if value_score >= 6:
            return "ğŸ”¥ é«˜ä»·å€¼èµ„è®¯ï¼Œå¯¹è¡Œä¸šå½±å“é‡å¤§"
        elif value_score >= 3:
            return "ğŸŸ¡ é‡è¦èµ„è®¯ï¼Œå€¼å¾—æŒç»­å…³æ³¨"
        elif value_score >= 0:
            return "ğŸ”µ ä¸€èˆ¬èµ„è®¯ï¼Œäº†è§£å³å¯"
        else:
            return "âšª å‚è€ƒä¿¡æ¯ï¼Œä»·å€¼æœ‰é™"
    
    def _generate_content_summary(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹æ¦‚è¿°"""
        # ç®€åŒ–å†…å®¹ï¼Œå»é™¤HTMLæ ‡ç­¾å’Œå¤šä½™ç©ºæ ¼
        import re
        clean_content = re.sub(r'<[^>]+>', '', content)
        clean_content = ' '.join(clean_content.split())
        
        # æˆªå–å‰300ä¸ªå­—ç¬¦ä½œä¸ºæ¦‚è¿°åŸºç¡€
        if len(clean_content) > 300:
            summary_base = clean_content[:300] + "..."
        else:
            summary_base = clean_content
            
        return summary_base
    
    def _extract_key_points(self, content: str, keywords: List[str]) -> str:
        """æå–æ ¸å¿ƒè¦ç‚¹"""
        content_lower = content.lower()
        points = []
        
        # æ ¹æ®ä¸åŒç±»å‹çš„AIå…³é”®è¯æå–è¦ç‚¹
        if any(kw in ['ChatGPT', 'GPT', 'å¤§è¯­è¨€æ¨¡å‹', 'LLM', 'ç”Ÿæˆå¼AI'] for kw in keywords):
            if 'gpt' in content_lower or 'chatgpt' in content_lower:
                points.append("â€¢ æ¶‰åŠå¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯å‘å±•")
            if any(word in content_lower for word in ['å‡çº§', 'update', 'æ›´æ–°', 'å‘å¸ƒ']):
                points.append("â€¢ å¯èƒ½åŒ…å«æŠ€æœ¯å‡çº§æˆ–æ–°ç‰ˆæœ¬å‘å¸ƒ")
                
        elif any(kw in ['æœºå™¨å­¦ä¹ ', 'machine learning', 'ML', 'æ·±åº¦å­¦ä¹ '] for kw in keywords):
            if any(word in content_lower for word in ['ç®—æ³•', 'algorithm', 'æ¨¡å‹', 'model']):
                points.append("â€¢ æ¶‰åŠæœºå™¨å­¦ä¹ ç®—æ³•æˆ–æ¨¡å‹æ”¹è¿›")
            if any(word in content_lower for word in ['è®­ç»ƒ', 'training', 'æ•°æ®', 'data']):
                points.append("â€¢ å¯èƒ½è®¨è®ºè®­ç»ƒæ–¹æ³•æˆ–æ•°æ®å¤„ç†æŠ€æœ¯")
                
        elif any(kw in ['è‡ªåŠ¨é©¾é©¶', 'æ™ºèƒ½é©¾é©¶', 'autonomous', 'æ±½è½¦'] for kw in keywords):
            if any(word in content_lower for word in ['æµ‹è¯•', 'test', 'è·¯æµ‹', 'è¯•éªŒ']):
                points.append("â€¢ å¯èƒ½æ¶‰åŠè‡ªåŠ¨é©¾é©¶æµ‹è¯•æˆ–è¯•éªŒ")
            if any(word in content_lower for word in ['å®‰å…¨', 'safety', 'äº‹æ•…', 'æ³•è§„']):
                points.append("â€¢ å…³æ³¨è‡ªåŠ¨é©¾é©¶å®‰å…¨æ€§æˆ–æ³•è§„é—®é¢˜")
                
        elif any(kw in ['æœºå™¨äºº', 'robot', 'robotics'] for kw in keywords):
            if any(word in content_lower for word in ['æœåŠ¡', 'service', 'åº”ç”¨', 'application']):
                points.append("â€¢ æ¶‰åŠæœºå™¨äººæœåŠ¡åº”ç”¨")
            if any(word in content_lower for word in ['åˆ¶é€ ', 'manufacturing', 'å·¥ä¸š', 'industrial']):
                points.append("â€¢ å¯èƒ½å…³æ³¨å·¥ä¸šæœºå™¨äººå‘å±•")
                
        # é€šç”¨è¦ç‚¹æ£€æµ‹
        if any(word in content_lower for word in ['çªç ´', 'breakthrough', 'åˆ›æ–°', 'innovation']):
            points.append("â€¢ å¯èƒ½åŒ…å«æŠ€æœ¯çªç ´æˆ–åˆ›æ–°")
        if any(word in content_lower for word in ['æŠ•èµ„', 'investment', 'èèµ„', 'funding']):
            points.append("â€¢ æ¶‰åŠæŠ•èµ„æˆ–èèµ„ä¿¡æ¯")
        if any(word in content_lower for word in ['ç«äº‰', 'competition', 'å¸‚åœº', 'market']):
            points.append("â€¢ æ¶‰åŠå¸‚åœºç«äº‰æ€åŠ¿")
        if any(word in content_lower for word in ['æ”¿ç­–', 'policy', 'ç›‘ç®¡', 'regulation']):
            points.append("â€¢ å¯èƒ½æ¶‰åŠæ”¿ç­–æˆ–ç›‘ç®¡åŠ¨æ€")
            
        if not points:
            points.append("â€¢ åŒ…å«AIé¢†åŸŸç›¸å…³æŠ€æœ¯æˆ–å¸‚åœºä¿¡æ¯")
            
        return '\n'.join(points[:4])  # æœ€å¤šæ˜¾ç¤º4ä¸ªè¦ç‚¹
    
    def _analyze_impact(self, content: str, keywords: List[str]) -> str:
        """åˆ†ææ½œåœ¨å½±å“"""
        content_lower = content.lower()
        
        # æŠ€æœ¯å½±å“åˆ†æ
        if any(kw in ['ChatGPT', 'GPT', 'å¤§è¯­è¨€æ¨¡å‹'] for kw in keywords):
            if any(word in content_lower for word in ['èƒ½åŠ›', 'capability', 'æ€§èƒ½', 'performance']):
                return "å¯èƒ½å¯¹AIå¯¹è¯ç³»ç»Ÿå’Œè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸäº§ç”Ÿç§¯æå½±å“"
            elif any(word in content_lower for word in ['é£é™©', 'risk', 'é—®é¢˜', 'problem']):
                return "éœ€è¦å…³æ³¨å¤§è¯­è¨€æ¨¡å‹çš„æ½œåœ¨é£é™©å’Œä¼¦ç†é—®é¢˜"
                
        elif any(kw in ['è‡ªåŠ¨é©¾é©¶', 'æ™ºèƒ½é©¾é©¶'] for kw in keywords):
            if any(word in content_lower for word in ['å®‰å…¨', 'safety']):
                return "å¯¹äº¤é€šå®‰å…¨å’Œæ™ºèƒ½äº¤é€šç³»ç»Ÿå‘å±•å…·æœ‰é‡è¦æ„ä¹‰"
            elif any(word in content_lower for word in ['å•†ä¸šåŒ–', 'commercial']):
                return "å¯èƒ½æ¨åŠ¨è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„å•†ä¸šåŒ–è¿›ç¨‹"
                
        elif any(kw in ['æœºå™¨äºº'] for kw in keywords):
            if any(word in content_lower for word in ['æœåŠ¡', 'service']):
                return "å¯èƒ½æ”¹å˜æœåŠ¡è¡Œä¸šçš„å·¥ä½œæ¨¡å¼å’Œæ•ˆç‡"
            elif any(word in content_lower for word in ['åˆ¶é€ ', 'manufacturing']):
                return "å¯¹åˆ¶é€ ä¸šè‡ªåŠ¨åŒ–å’Œæ•ˆç‡æå‡æœ‰ç§¯æä½œç”¨"
                
        # é€šç”¨å½±å“åˆ†æ
        if any(word in content_lower for word in ['çªç ´', 'breakthrough']):
            return "æŠ€æœ¯çªç ´å¯èƒ½æ¨åŠ¨æ•´ä¸ªAIè¡Œä¸šçš„å‘å±•è¿›æ­¥"
        elif any(word in content_lower for word in ['æŠ•èµ„', 'investment', 'èèµ„']):
            return "èµ„æœ¬åŠ¨å‘å¯èƒ½å½±å“AIäº§ä¸šçš„å‘å±•æ–¹å‘å’Œé€Ÿåº¦"
        elif any(word in content_lower for word in ['æ”¿ç­–', 'policy', 'ç›‘ç®¡']):
            return "æ”¿ç­–å˜åŒ–å¯èƒ½å¯¹AIè¡Œä¸šå‘å±•äº§ç”Ÿè§„èŒƒå’Œå¼•å¯¼ä½œç”¨"
        else:
            return "ä¸ºAIæŠ€æœ¯å‘å±•å’Œåº”ç”¨æä¾›æœ‰ä»·å€¼çš„å‚è€ƒä¿¡æ¯"
    
    def summarize_articles_batch(self, articles: List[Dict]) -> List[Dict]:
        """æ‰¹é‡å¯¹æ–‡ç« è¿›è¡Œæ€»ç»“"""
        try:
            self.logger.info(f"å¼€å§‹å¯¹ {len(articles)} ç¯‡æ–‡ç« è¿›è¡Œæ€»ç»“...")
            
            for article in articles:
                article['ai_summary'] = self.summarize_article(article)
            
            self.logger.info(f"æ–‡ç« æ€»ç»“å®Œæˆ")
            return articles
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡æ–‡ç« æ€»ç»“å¤±è´¥: {str(e)}")
            return articles

if __name__ == "__main__":
    # æµ‹è¯•æŠ“å–åŠŸèƒ½
    scraper = AINewsScraper()
    articles = scraper.get_ai_news()
    filtered_articles = scraper.filter_ai_keywords(articles)
    
    print(f"è·å–åˆ° {len(filtered_articles)} ç¯‡AIèµ„è®¯")
    for i, article in enumerate(filtered_articles[:5]):
        print(f"\n{i+1}. {article['title']}")
        print(f"   æ¥æº: {article['source']}")
        print(f"   é“¾æ¥: {article['link']}")
        print(f"   æ—¶é—´: {article['published']}")
        print(f"   æ‘˜è¦: {article['summary'][:100]}...")